
@misc{howell_predictive_2022,
	title = {Predictive {Sampling}: {Real}-time {Behaviour} {Synthesis} with {MuJoCo}},
	shorttitle = {Predictive {Sampling}},
	url = {http://arxiv.org/abs/2212.00541},
	abstract = {We introduce MuJoCo MPC (MJPC), an open-source, interactive application and software framework for real-time predictive control, based on MuJoCo physics. MJPC allows the user to easily author and solve complex robotics tasks, and currently supports three shooting-based planners: derivative-based iLQG and Gradient Descent, and a simple derivative-free method we call Predictive Sampling. Predictive Sampling was designed as an elementary baseline, mostly for its pedagogical value, but turned out to be surprisingly competitive with the more established algorithms. This work does not present algorithmic advances, and instead, prioritises performant algorithms, simple code, and accessibility of model-based methods via intuitive and interactive software. MJPC is available at: github.com/deepmind/mujoco\_mpc, a video summary can be viewed at: dpmd.ai/mjpc.},
	urldate = {2024-04-22},
	publisher = {arXiv},
	author = {Howell, Taylor and Gileadi, Nimrod and Tunyasuvunakool, Saran and Zakka, Kevin and Erez, Tom and Tassa, Yuval},
	month = dec,
	year = {2022},
	note = {arXiv:2212.00541 [cs, eess]},
	keywords = {Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
	file = {arXiv Fulltext PDF:/home/nelorth/Zotero/storage/6YKE86JG/Howell et al. - 2022 - Predictive Sampling Real-time Behaviour Synthesis.pdf:application/pdf;arXiv.org Snapshot:/home/nelorth/Zotero/storage/UWXL7M79/2212.html:text/html},
}

@inproceedings{liu_partially_2023,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Partially {Observable} {Multi}-agent {RL} with ({Quasi}-){Efficiency}: {The} {Blessing} of {Information} {Sharing}},
	volume = {202},
	url = {https://proceedings.mlr.press/v202/liu23ay.html},
	abstract = {We study provable multi-agent reinforcement learning (MARL) in the general framework of partially observable stochastic games (POSGs). To circumvent the known hardness results and the use of computationally intractable oracles, we propose to leverage the potential {\textless}em{\textgreater}information-sharing{\textless}/em{\textgreater} among agents, a standard practice in empirical MARL and a common model for multi-agent control systems with communications. We first establish several computation complexity results to justify the necessity of information-sharing, as well as the observability assumption that has enabled quasi-efficient single-agent RL with partial observations, for computational efficiency in solving POSGs. We then propose to further {\textless}em{\textgreater}approximate{\textless}/em{\textgreater} the shared common information to construct an approximate model of the POSG, in which planning an approximate equilibrium (in terms of solving the original POSG) can be quasi-efficient, i.e., of quasi-polynomial-time, under the aforementioned assumptions. Furthermore, we develop a partially observable MARL algorithm that is both statistically and computationally quasi-efficient. We hope our study can open up the possibilities of leveraging and even designing different {\textless}em{\textgreater}information structures{\textless}/em{\textgreater}, for developing both sample- and computation-efficient partially observable MARL.},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Liu, Xiangyu and Zhang, Kaiqing},
	editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	month = jul,
	year = {2023},
	pages = {22370--22419},
}

@inproceedings{schroeder_de_witt_multi-agent_2019,
	title = {Multi-{Agent} {Common} {Knowledge} {Reinforcement} {Learning}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/f968fdc88852a4a3a27a81fe3f57bfc5-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Schroeder de Witt, Christian and Foerster, Jakob and Farquhar, Gregory and Torr, Philip and Boehmer, Wendelin and Whiteson, Shimon},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d' and Fox, E. and Garnett, R.},
	year = {2019},
}

@inproceedings{lanctot_unified_2017,
	title = {A {Unified} {Game}-{Theoretic} {Approach} to {Multiagent} {Reinforcement} {Learning}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3323fe11e9595c09af38fe67567a9394-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lanctot, Marc and Zambaldi, Vinicius and Gruslys, Audrunas and Lazaridou, Angeliki and Tuyls, Karl and Perolat, Julien and Silver, David and Graepel, Thore},
	editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
}

@inproceedings{muglich_equivariant_2022,
	title = {Equivariant {Networks} for {Zero}-{Shot} {Coordination}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/29e4b51d45dc8f534260adc45b587363-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Muglich, Darius and Schroeder de Witt, Christian and van der Pol, Elise and Whiteson, Shimon and Foerster, Jakob},
	editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
	year = {2022},
	pages = {6410--6423},
}

@inproceedings{peng_facmac_2021,
	title = {{FACMAC}: {Factored} {Multi}-{Agent} {Centralised} {Policy} {Gradients}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/65b9eea6e1cc6bb9f0cd2a47751a186f-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Peng, Bei and Rashid, Tabish and Schroeder de Witt, Christian and Kamienny, Pierre-Alexandre and Torr, Philip and Boehmer, Wendelin and Whiteson, Shimon},
	editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P. S. and Vaughan, J. Wortman},
	year = {2021},
	pages = {12208--12221},
}

@inproceedings{letcher_stable_2019,
	title = {Stable {Opponent} {Shaping} in {Differentiable} {Games}},
	url = {https://openreview.net/forum?id=SyGjjsC5tQ},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Letcher, Alistair and Foerster, Jakob and Balduzzi, David and Rocktäschel, Tim and Whiteson, Shimon},
	year = {2019},
}

@article{foerster_counterfactual_2018,
	title = {Counterfactual {Multi}-{Agent} {Policy} {Gradients}},
	volume = {32},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11794},
	doi = {10.1609/aaai.v32i1.11794},
	abstract = {Many real-world problems, such as network packet routing and the coordination of autonomous vehicles, are naturally modelled as cooperative multi-agent systems. There is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems. To this end, we propose a new multi-agent actor-critic method called counterfactual multi-agent (COMA) policy gradients. COMA uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents' policies. In addition, to address the challenges of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent's action, while keeping the other agents' actions fixed. COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass. We evaluate COMA in the testbed of StarCraft unit micromanagement, using a decentralised variant with significant partial observability. COMA significantly improves average performance over other multi-agent actor-critic methods in this setting, and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state.},
	number = {1},
	urldate = {2024-04-23},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Foerster, Jakob and Farquhar, Gregory and Afouras, Triantafyllos and Nardelli, Nantas and Whiteson, Shimon},
	month = apr,
	year = {2018},
	file = {Full Text:/home/nelorth/Zotero/storage/PVUQBK4I/Foerster et al. - 2018 - Counterfactual Multi-Agent Policy Gradients.pdf:application/pdf},
}

@misc{rostel_factory_2024,
	title = {Factory {Manipulation} {Environment} ({MuJoCo})},
	url = {https://www.youtube.com/watch?v=IpwQPmgOLW0},
	abstract = {Implementation of a factory manipulation environment, built using MuJoCo. 
The baseline control policy shown is using inverse kinematics.},
	urldate = {2024-04-23},
	author = {Röstel, Lennart},
	month = apr,
	year = {2024},
}

@article{schroeder_de_witt_deep_2020,
	title = {Deep {Multi}-{Agent} {Reinforcement} {Learning} for {Decentralized} {Continuous} {Cooperative} {Control}},
	url = {https://www.semanticscholar.org/paper/Deep-Multi-Agent-Reinforcement-Learning-for-Control-Witt-Peng/85eb9d305dc5fc5cb3f1553f4595a1e3d2913a90},
	abstract = {Centralised training with decentralised execution (CTDE) is an important learning paradigm in multi-agent reinforcement learning (MARL). To make progress in CTDE, we introduce Multi-Agent Mujoco, a novel benchmark suite that, unlike StarCraft II, the predominant benchmark environment, applies to continuous robotic control tasks. To demonstrate the utility of Multi-Agent Mujoco, we present a range of benchmark results on this new suite, including comparing the state-of-the-art actor-critic method MADDPG against two novel variants of existing methods. These new methods outperform MADDPG on several Multi-Agent Mujoco tasks. In addition, we show that factorisation is key to performance, but other algorithmic choices are not. This motivates the necessity of extending the study of value factorisations from \$Q\$-learning to actor-critic algorithms.},
	urldate = {2024-04-23},
	journal = {ArXiv},
	author = {Schroeder de Witt, Christian and Peng, Bei and Kamienny, Pierre-Alexandre and Torr, Philip H. S. and Böhmer, Wendelin and Whiteson, Shimon},
	month = mar,
	year = {2020},
}

@article{whiteson_cooperative_2018,
	title = {Cooperative {Multi}-{Agent} {Reinforcement} {Learning}},
	url = {https://www.microsoft.com/en-us/research/uploads/prod/2018/03/Shimon-Whiteson.pdf},
	language = {en},
	journal = {https://www.microsoft.com/en-us/research/uploads/prod/2018/03/Shimon-Whiteson.pdf},
	author = {Whiteson, Shimon},
	year = {2018},
	file = {Whiteson - Cooperative Multi-Agent Reinforcement Learning.pdf:/home/nelorth/Zotero/storage/G62JLARR/Whiteson - Cooperative Multi-Agent Reinforcement Learning.pdf:application/pdf},
}

@inproceedings{laskin_unsupervised_2022,
	title = {Unsupervised {Reinforcement} {Learning} with {Contrastive} {Intrinsic} {Control}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/debf482a7dbdc401f9052dbe15702837-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Laskin, Michael and Liu, Hao and Peng, Xue Bin and Yarats, Denis and Rajeswaran, Aravind and Abbeel, Pieter},
	editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
	year = {2022},
	pages = {34478--34491},
}

@inproceedings{laskin_urlb_2021,
	title = {{URLB}: {Unsupervised} {Reinforcement} {Learning} {Benchmark}},
	url = {https://openreview.net/forum?id=lwrPkQP_is},
	booktitle = {Thirty-fifth {Conference} on {Neural} {Information} {Processing} {Systems} {Datasets} and {Benchmarks} {Track} ({Round} 2)},
	author = {Laskin, Michael and Yarats, Denis and Liu, Hao and Lee, Kimin and Zhan, Albert and Lu, Kevin and Cang, Catherine and Pinto, Lerrel and Abbeel, Pieter},
	year = {2021},
}

@inproceedings{evans_visualization_2024,
	title = {Visualization of {Bipartite} {Graphs} in {Limited} {Window} {Size}},
	booktitle = {International {Conference} on {Current} {Trends} in {Theory} and {Practice} of {Computer} {Science}},
	publisher = {Springer},
	author = {Evans, William and Köck, Kassian and Kobourov, Stephen},
	year = {2024},
	pages = {198--210},
}

@inproceedings{yu_multi-robot_2017,
	address = {Macau},
	title = {Multi-robot coordination for high-speedpick-and-place tasks},
	isbn = {978-1-5386-3742-5},
	url = {http://ieeexplore.ieee.org/document/8324670/},
	doi = {10.1109/ROBIO.2017.8324670},
	urldate = {2024-05-08},
	booktitle = {2017 {IEEE} {International} {Conference} on {Robotics} and {Biomimetics} ({ROBIO})},
	publisher = {IEEE},
	author = {Yu, Chao and Liu, Xin-Jun and Qiao, Fei and Xie, Fugui},
	month = dec,
	year = {2017},
	pages = {1743--1750},
}

@incollection{carbonell_coordinated_1998,
	address = {Berlin, Heidelberg},
	title = {Coordinated motion of two robot arms for real applications},
	volume = {1416},
	isbn = {978-3-540-64574-0 978-3-540-69350-5},
	url = {http://link.springer.com/10.1007/3-540-64574-8_398},
	urldate = {2024-05-08},
	booktitle = {Tasks and {Methods} in {Applied} {Artificial} {Intelligence}},
	publisher = {Springer Berlin Heidelberg},
	author = {Pérez-Francisco, Miguel and Del Pobil, Angel P. and Martfnez-Salvador, Begoña},
	editor = {Carbonell, Jaime G. and Siekmann, Jörg and Goos, G. and Hartmanis, J. and Van Leeuwen, J. and Pasqual Del Pobil, Angel and Mira, José and Ali, Moonis},
	year = {1998},
	doi = {10.1007/3-540-64574-8_398},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {122--131},
}

@article{bozma_multirobot_2012,
	title = {Multirobot coordination in pick-and-place tasks on a moving conveyor},
	volume = {28},
	issn = {07365845},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0736584511001396},
	doi = {10.1016/j.rcim.2011.12.001},
	language = {en},
	number = {4},
	urldate = {2024-05-08},
	journal = {Robotics and Computer-Integrated Manufacturing},
	author = {Bozma, H. Işıl and Kalalıoğlu, M.E.},
	month = aug,
	year = {2012},
	pages = {530--538},
}

@article{han_toward_2020,
	title = {Toward {Fast} and {Optimal} {Robotic} {Pick}-and-{Place} on a {Moving} {Conveyor}},
	volume = {5},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {2377-3766, 2377-3774},
	url = {https://ieeexplore.ieee.org/document/8938744/},
	doi = {10.1109/LRA.2019.2961605},
	number = {2},
	urldate = {2024-05-08},
	journal = {IEEE Robotics and Automation Letters},
	author = {Han, Shuai D. and Feng, Si Wei and Yu, Jingjin},
	month = apr,
	year = {2020},
	pages = {446--453},
	file = {Submitted Version:/home/nelorth/Zotero/storage/5MF56KP6/Han et al. - 2020 - Toward Fast and Optimal Robotic Pick-and-Place on .pdf:application/pdf},
}

@article{nguyen_deep_2020,
	title = {Deep {Reinforcement} {Learning} for {Multiagent} {Systems}: {A} {Review} of {Challenges}, {Solutions}, and {Applications}},
	volume = {50},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {2168-2267, 2168-2275},
	shorttitle = {Deep {Reinforcement} {Learning} for {Multiagent} {Systems}},
	url = {https://ieeexplore.ieee.org/document/9043893/},
	doi = {10.1109/TCYB.2020.2977374},
	number = {9},
	urldate = {2024-05-08},
	journal = {IEEE Transactions on Cybernetics},
	author = {Nguyen, Thanh Thi and Nguyen, Ngoc Duy and Nahavandi, Saeid},
	month = sep,
	year = {2020},
	pages = {3826--3839},
	file = {Submitted Version:/home/nelorth/Zotero/storage/ZPT9BEDE/Nguyen et al. - 2020 - Deep Reinforcement Learning for Multiagent Systems.pdf:application/pdf},
}

@inproceedings{lan_towards_2021,
	address = {Prague, Czech Republic},
	title = {Towards {Pick} and {Place} {Multi} {Robot} {Coordination} {Using} {Multi}-agent {Deep} {Reinforcement} {Learning}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-66540-469-3},
	url = {https://ieeexplore.ieee.org/document/9376433/},
	doi = {10.1109/ICARA51699.2021.9376433},
	urldate = {2024-05-08},
	booktitle = {2021 7th {International} {Conference} on {Automation}, {Robotics} and {Applications} ({ICARA})},
	publisher = {IEEE},
	author = {Lan, Xi and Qiao, Yuansong and Lee, Brian},
	month = feb,
	year = {2021},
	pages = {85--89},
}

@inproceedings{lan_coordination_2022,
	address = {Paris, France},
	title = {Coordination of a {Multi} {Robot} {System} for {Pick} and {Place} {Using} {Reinforcement} {Learning}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66548-194-6},
	url = {https://ieeexplore.ieee.org/document/10027088/},
	doi = {10.1109/CompAuto55930.2022.00024},
	urldate = {2024-05-08},
	booktitle = {2022 2nd {International} {Conference} on {Computers} and {Automation} ({CompAuto})},
	publisher = {IEEE},
	author = {Lan, Xi and Qiao, Yuansong and Lee, Brian},
	month = aug,
	year = {2022},
	pages = {87--92},
}

@inproceedings{ji_ace_2024,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {{ACE}: {Off}-{Policy} {Actor}-{Critic} with {Causality}-{Aware} {Entropy} {Regularization}},
	volume = {235},
	url = {https://proceedings.mlr.press/v235/ji24b.html},
	abstract = {The varying significance of distinct primitive behaviors during the policy learning process has been overlooked by prior model-free RL algorithms. Leveraging this insight, we explore the causal relationship between different action dimensions and rewards to evaluate the significance of various primitive behaviors during training. We introduce a causality-aware entropy term that effectively identifies and prioritizes actions with high potential impacts for efficient exploration. Furthermore, to prevent excessive focus on specific primitive behaviors, we analyze the gradient dormancy phenomenon and introduce a dormancy-guided reset mechanism to further enhance the efficacy of our method. Our proposed algorithm, \textbf{ACE}: Off-policy \textbf{A}ctor-critic with \textbf{C}ausality-aware \textbf{E}ntropy regularization, demonstrates a substantial performance advantage across 29 diverse continuous control tasks spanning 7 domains compared to model-free RL baselines, which underscores the effectiveness, versatility, and efficient sample efficiency of our approach. Benchmark results and videos are available at https://ace-rl.github.io/.},
	booktitle = {Proceedings of the 41st {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ji, Tianying and Liang, Yongyuan and Zeng, Yan and Luo, Yu and Xu, Guowei and Guo, Jiawei and Zheng, Ruijie and Huang, Furong and Sun, Fuchun and Xu, Huazhe},
	editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
	month = jul,
	year = {2024},
	pages = {21620--21647},
}

@article{raffin_stable-baselines3_2021,
	title = {Stable-{Baselines3}: {Reliable} {Reinforcement} {Learning} {Implementations}},
	volume = {22},
	url = {http://jmlr.org/papers/v22/20-1364.html},
	number = {268},
	journal = {Journal of Machine Learning Research},
	author = {Raffin, Antonin and Hill, Ashley and Gleave, Adam and Kanervisto, Anssi and Ernestus, Maximilian and Dormann, Noah},
	year = {2021},
	pages = {1--8},
}
